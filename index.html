<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training.">
  <meta name="keywords" content="VLA, Vision Language Action Model, TAD, Temporal Action Detection, Manufacturing, Robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>From Observation to Action: Latent Action-based Primitive Segmentation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://jiajiezhang7.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

          </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=XcjYjJYAAAAJ&hl=en">Jiajie Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=de&user=Y2olJ9kAAAAJ">Sören Schwertfeger</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=2J5RWqcAAAAJ&hl=en">Alexander Kleiner</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ShanghaiTech University,</span>
            <span class="author-block"><sup>2</sup>Hangzhou Dianzi University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2511.21428"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2511.21428"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/jiajiezhang7/LAPS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/action_segmentor_teaser.png" alt="Teaser" style="width: 100%;">
      <h2 class="subtitle has-text-centered">
        Our framework automatically segments continuous industrial videos into semantically coherent action primitives for VLA pre-training.
      </h2>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training.
          </p>
          <p>
            Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel "Latent Action Energy" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training.
          </p>
          <p>
            Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video" style="background: #f5f5f5; display: flex; align-items: center; justify-content: center; min-height: 300px;">
          <!-- TODO: Replace with actual YouTube embed -->
          <!-- <iframe src="https://www.youtube.com/embed/YOUR_VIDEO_ID?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          <p style="color: #888; font-size: 1.2em;">Video Coming Soon</p>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Method Pipeline. -->
      <div class="column is-full-width">
        <div class="content">
          <h2 class="title is-3">Method Pipeline</h2>
          <p>Overview of the LAPS pipeline:</p>
          <p>
            <b>(1) Motion Tracking:</b> Extracts motion keypoints from raw video using a point tracker.
          </p>
          <p>
            <b>(2) Action Detection & Segmentation:</b> Generates a latent vector stream via a motion tokenizer and identifies action boundaries to segment latent vectors, video clips, and action codes.
          </p>
          <p>
            <b>(3) Semantic Action Clustering:</b> Groups the segmented latent vectors into meaningful semantic action clusters.
          </p>
          <img src="./static/images/pipeline.jpeg" alt="Method Pipeline" style="width: 100%;">
        </div>
      </div>
    </div>
    <!--/ Method Pipeline. -->

    <!-- Clustering Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Clustering Results</h2>
        <p class="content has-text-justified">
          Visualization of discovered action primitives through clustering. Each cluster represents a semantically coherent action type.
        </p>
        
        <h3 class="title is-4"> Exocentric View Dataset</h3>
        <div class="content has-text-centered">
          <video autoplay loop muted playsinline style="width: 100%;">
            <source src="./static/videos/D01_Cluster.m4v" type="video/mp4">
          </video>
        </div>

        <h3 class="title is-4"> Top-down View Dataset</h3>
        <div class="content has-text-centered">
          <video autoplay loop muted playsinline style="width: 100%;">
            <source src="./static/videos/D02_Cluster.m4v" type="video/mp4">
          </video>
        </div>

      </div>
    </div>
    <!--/ Clustering Results. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <h3 class="title is-4">Foundations: Latent Action Representations</h3>
          <p>
            <a href="https://arxiv.org/abs/2506.14198"><b>AMPLIFY: Actionless Motion Priors for Robot Learning</b></a> <i>Collins et al., 2025</i><br>
            The architectural foundation for our <b>Motion Tokenizer</b>. We adapt their quantized autoencoder to define "Action Energy" for temporal segmentation rather than direct policy learning.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2410.11758"><b>LAPA: Latent Action Pretraining from Videos</b></a> <i>Ye et al., 2024</i><br>
            A key precedent in latent pre-training. While LAPA focuses on reconstruction objectives, our work addresses the upstream challenge of <b>automatically segmenting</b> primitives from continuous, unstructured streams.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2312.10812"><b>LAPO: Learning to Act without Actions</b></a> <i>Schmidt & Jiang, 2023</i><br>
            Demonstrates the power of latent spaces. We advance this by moving away from pixel-level prediction (which captures noise) to a semantic <b>behavioral intent</b> metric for boundary detection.
          </p>

          <h3 class="title is-4">Context & Tools</h3>
          <p>
            <a href="https://arxiv.org/abs/2406.09246"><b>OpenVLA: An Open-Source Vision-Language-Action Model</b></a> <i>Kim et al., 2024</i><br>
            A representative generalist VLA. Our pipeline aims to solve the critical "data sourcing bottleneck" for training such models in industrial domains.
          </p>
          <p>
            <a href="https://github.com/facebookresearch/co-tracker"><b>CoTracker3: Point Tracking by Pseudo-labelling</b></a> <i>Karaev et al., 2025</i><br>
            The state-of-the-art point tracker utilized in our pipeline to extract dense motion dynamics from raw industrial footage.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zhang2025observationactionlatentactionbased,
  title={From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings}, 
  author={Jiajie Zhang and Sören Schwertfeger and Alexander Kleiner},
  year={2025},
  eprint={2511.21428},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2511.21428}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://jiajiezhang7.github.io/" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
