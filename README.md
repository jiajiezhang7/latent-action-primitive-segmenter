# From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings

This is the project page for our paper on unsupervised action primitive segmentation from industrial videos.

**[Paper](https://arxiv.org/abs/2511.21428)** | **[Code](https://github.com/jiajiezhang7/latent_rep_from_video)**

## Abstract

We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel "Latent Action Energy" metric to discover and segment semantically coherent action primitives.

## Citation

If you find this work useful, please cite:
```bibtex
@misc{zhang2025observationactionlatentactionbased,
  title={From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings}, 
  author={Jiajie Zhang and SÃ¶ren Schwertfeger and Alexander Kleiner},
  year={2025},
  eprint={2511.21428},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2511.21428}, 
}
```

## Website License
<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
